{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404bb764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk spacy\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea0c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ba7bd",
   "metadata": {},
   "source": [
    "Exploring and cleaning the Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612c461",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"complaint_data.csv\",on_bad_lines='skip',quoting=csv.QUOTE_NONE)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b860952",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df_cleaned=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f466e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cleaned.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557dfd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.dropna(subset=['Consumer complaint narrative'])\n",
    "df_cleaned=df_cleaned.drop((df_cleaned.loc[df_cleaned['Consumer complaint narrative'].str.len()<10]).index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facfc4f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date_pattern = r'\\d{2}/\\d{2}/\\d{4}'\n",
    "valid_dates=df_cleaned['Date received'].str.contains(date_pattern, regex=True,na=False)\n",
    "df_cleaned = df_cleaned[valid_dates]\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe1b47",
   "metadata": {},
   "source": [
    "I think this data is extracted by scraping. So to get all the issue in one column i will have to concat some columns in Consumer Complaint Narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c117240",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cleaned=df_cleaned[['Product','Sub-product', 'Issue', 'Sub-issue','Consumer complaint narrative', 'Company public response', 'Company response to consumer']]\n",
    "df_cleaned['final_problem']=df_cleaned['Consumer complaint narrative']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a17701",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_public_response=df_cleaned.loc[df_cleaned['Company public response'].str.len()>100]\n",
    "df_cleaned.loc[wrong_public_response.index, 'final_problem'] =wrong_public_response['final_problem']+wrong_public_response['Company public response']\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    print(index,row['final_problem'])\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d64210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.drop(columns=['Consumer complaint narrative','Company public response', 'Company response to consumer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d105500",
   "metadata": {},
   "source": [
    "After getting the rows which we are gonna work upon, we are gonna prepare the text for topic modelling.\n",
    "Make the text lowercase\n",
    "Remove text in square brackets\n",
    "Remove punctuation\n",
    "Remove words containing numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ba1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    text = text.lower() # text to lowercase\n",
    "    text = re.sub(r'\\s\\{\\$\\S*', '',text) # Remove text within curly braces\n",
    "    text = re.sub(r'\\n', '', text) # Remove line breaks\n",
    "    text = re.sub(r'\\(\\w*\\)', '', text) #remove text within braces\n",
    "    text = re.sub(r'(\\W\\s)|(\\W$)|(\\W\\d*)', ' ',text) # Remove punctuation\n",
    "    text = re.sub(r'x+((/xx)*/\\d*\\s*)|x*', '',text) #Remove date\n",
    "    text = re.sub(r'\\d+\\s', '', text) #Remove other numerical values\n",
    "    text = re.sub(r' +', ' ',text) #Remove unnecessary white spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a9eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_cleaned['final_problem'] = df_cleaned['final_problem'].apply(clean_data)\n",
    "df_cleaned.iloc[:,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1273ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nlp.Defaults.stop_words\n",
    "def lemmatization(texts):\n",
    "    lemma_sentences = []\n",
    "    for doc in tqdm(nlp.pipe(texts)):\n",
    "        sent = [token.lemma_ for token in doc if token.text not in set(stopwords)]\n",
    "        lemma_sentences.append(' '.join(sent))\n",
    "    return lemma_sentences\n",
    "\n",
    "df_cleaned['lemmatised_description']=lemmatization(df_cleaned['final_problem'])\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016eeb36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
